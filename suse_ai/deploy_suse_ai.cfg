############################################################################
# Source In Additional Config Files
############################################################################

source ../deploy_rke2.cfg


############################################################################
#   SUSE AI Common Variables
############################################################################

# Namspace that Will Contain All SUSE AI Objects
#
export SUSE_AI_NAMESPACE=suse-ai


############################################################################
#   SUSE Private AI Variables
#
#   NOTE: These apply when deploying the SUSE AI stack using the
#         suse-ai-deployer chart. Individual components are configured
#         in their own subsections of this file 
#         (i.e. milvus, ollama, owui, pytorch, etc.).
############################################################################

# Version of SUSE AI Deployer Chart to Install
#
# If left empty the latest version will be installed.
#
export SUSE_AI_DEPLOYER_VERSION=


# Enable Milvus in the SUSE Private AI Deployer
#
# Default: false
#
export SUSE_PRIVATE_AI_MILVUS_ENABLED=true


# Enable PyTorch in the SUSE Private AI Deployer
#
# Default: false
#
export SUSE_PRIVATE_AI_PYTORCH_ENABLED=false


############################################################################
#   Milvus Variables
############################################################################

# Version of Milvus to Install
#
# If left empty the latest version will be installed.
#
export MILVUS_VERSION=4.2.2


# Run Milvus in clustered or standalone mode
#
# If there is only a single cluster node set this to: false
#
# If you have multiple cluster nodes but still want to run in standalone mode
# set this to: false
#
# If set to "false" the only containers that are deployed are:
#  -milvus-etcd
#  -milvus-minio
#  -milvus-standalone
#
# Default: true
#
export MILVUS_CLUSTER_ENABLED=false


# Log level to use with Milvus
#
# Default: info
#
export MILVUS_LOGGING_LEVEL=info


# Where to store the Milvus logs
#
# Options:
#    emptyDir      (store logs in an emptyDir)
#    storageClass  (store logs in a PV of type storageClass)
#
# Defalult: emptyDir
#
export MILVUS_LOGGING_STORAGE=emptyDir


# Message queue to use with Milvus in Standalone Mode
#
# Standalone Mode:
#   Default: rocksmq
# 
#   Milvus can use kafka in standalone mode however,
#   if in doubt leave this set to: rocksmq
#
# Cluster Mode:
#   Default: kafka
#
export MILVUS_STANDALONE_MESSAGE_QUEUE=rocksmq


#========  Etcd Values  ========#

# Enable etcd
#
# Default: true
#
export MILVUS_ETCD_ENABLED=true


# Etcd Replica Count
#
# Set this t "1" if runing a single node cluster or running in standalone mode.
#
# Default: 3
#
export MILVUS_ETCD_REPLICA_COUNT=1


#======== Minio Values  ========#

# Enable MinIO
#
# Default: true
#
export MILVUS_MINIO_ENABLED=true


# Minio Admin Username
#
export MILVUS_MINIO_ROOT_USER=admin


# Minio Admin User Password
#
export MILVUS_MINIO_ROOT_USER_PASSWORD=adminminio


# Minio Mode
#
# Options: distributed, standalone
#
# Set this to "standalone" if MILVUS_CLUSTER_ENABLED=False
#
# Default: distributed
#
export MILVUS_MINIO_MODE=standalone


# Minio Replica Count
#
# Set this to "1" if running in stadalone mode.
#
# Default: 4
#
export MILVUS_MINIO_REPLICA_COUNT=1


# Volume Size for the Minio Persistent Volume
#
# Default: 500Gi
#
export MILVUS_MINIO_VOLUME_SIZE=100Gi


# The Amount of Memory that Minio Uses
#
# Default: 1024Mi
#
export MILVUS_MINIO_MEMORY=4096Mi


#========  Kafka Values  ========#

# Enable Kafka
#
# For single node/standalone Milvus Kafka is disabled (set to false)
#
# Default: true
#
export MILVUS_KAFKA_ENABLED=false


# Kaufka Replica Count
#
# Default: 3
#
export MILVUS_KAFKA_REPLICA_COUNT=3


# Kafka Volume Size
#
#  Default: 8Gi
#
export MILVUS_KAFKA_VOLUME_SIZE=8Gi


# Kafka Storage Class Name
#
# The name of the storage class that Kafka will use when creating 
# persistent volumes.
#
# Kafka PVs must be formatted with a file system other than Ext4, such as XFS.
#
# The default Longhorn storage class typically uses the Ext4 filesystem so a 
# new storage class must be created that uses XFS instead (unless the default 
# storage class was configured to use XFS). If this new storage class has
# been created then list it in this variable.
#
# If the variable is left empty then it uses the value in STORAGE_CLASS_NAME.
#
# Example: longhorn-xfs
#
export MILVUS_KAFKA_STORAGE_CLASS_NAME=


# Enabled Telemetry from Milvus
#
# Default: false
#
export MILVUS_TELEMETRY_ENABLED=true


############################################################################
#   Ollama Variables
############################################################################

# Version of Ollama to Install
#
# If left empty the latest version will be installed.
#
export OLAMA_VERSION=1.11.0


# URL to Access the Ollama API if Configuring an Ingress
#
export OLLAMA_INGRESS_HOST=${CLUSTER_NAME}.${DOMAIN_NAME}


# First LLM for Ollama to Download
#
# At minimum this must be set so that a model is downloaded during deployment.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_0=llama3.2:3b


# Second LLM for Ollama to Download
#
# Leave empty if you do not want an addtional model.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_1=gemma2:2b


# Third LLM for Ollama to Download
#
# Leave empty if you do not want an addtional model.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_2=bge-large


# Fourth LLM for Ollama to Download
#
# Leave empty if you do not want an addtional model.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_3=llama3.1-8b


# Fifth LLM for Ollama to Download
#
# Leave empty if you do not want an addtional model.
#
# This variable applys to both the ollama chart and the open-webui chart.
#
export OLLAMA_MODEL_4=


# Enable GPU for Ollama
#
# Default: false
#
export OLLAMA_GPU_ENABLED=true


# The GPU type to use for Ollama
#
# Default: nvidia
#
export OLLAMA_GPU_TYPE=nvidia


# The number of GPUs to use with Ollama
#
# Default: 1
#
export OLLAMA_GPU_NUMBER=1


# The NVIDIA Resource to use with NVIDIA GPU with Ollama
#
# Default: nvidia.com
#
export OLLAMA_GPU_NVIDIA_RESOURCE=nvidia.com/gpu


# The Runtime Class to use with Ollama
#
# Default: nvidia
#
export OLLAMA_RUNTIMECLASSNAME=nvidia


############################################################################
#   Open WebUI Variables
############################################################################

# Version of Open WebUI to Install
#
# If left empty the latest version will be installed.
#
export OWUI_VERSION=6.13.0


#========  TLS Values  ========#


# The source for the TLS certificates
#
# Options:
#      suse-private-ai (Cert-Manager will generate self-signed certificates)
#      letsEncrypt     (Cert-Manager uses letsEncrypt to generate certificates)
#      secret          (A K8s secret containing the certificate will be used)
#
export OWUI_TLS_SOURCE=suse-private-ai


# The email to use with letsEncrypt when it is used to generate certificates
#
# You must set this to something valid when using OWUI_TLS_SOURCE=letsEncrypt
#
export OWUI_TLS_EMAIL=admin@example.com


# The Lets Encrypt environment to create the certificates in
#
# Options:
#          staging  
#
export OWUI_TLS_LETSENCRYPT_ENVIRONMENT=staging


# The ingress class to use for the certificates
#
# Options: (none at the moment)
#
# Leave this variable empty unless otherwise instructed.
#
export OWUI_TLS_INGRESS_CLASS=


# Enable additional trusted certificates
#
# Options: [true|false]
# 
# Default: false
#
# This msut be set to 'true' when OWUI_TLS_SOURCE=secret
#
# If set to 'true' then a secret named tls-ca-additional must exist in the 
# SUSE AI namespace containing the additional certificates
#
# With other sources, if in doubt, set it to 'false'
#
export OWUI_TLS_ADDITIONAL_TRUSTED_CERTS=false


#========  Other Values  ========#


# Enable Open WebUI to install Ollama
#
# If enabled (set to True) Ollama will be deployed as part of the Open WebUI 
# deployment and that is the instance Open WebUI will use.
# If Ollama was previous deployed separatly and you want Open WebUI to use it
# instead of an instance it deploys set this to False.
#
# Default: True
#
export OWUI_OLLAMA_ENABLED=true


# URL Used to Access the Open WebUI Web Interface
#
export OWUI_INGRESS_HOST=${CLUSTER_NAME}.${DOMAIN_NAME}


############################################################################
#   Open WebUI Pipelines Variables
############################################################################

# Version of Open WebUI Pipelines to Install
#
# If left empty the latest version will be installed.
#
export OWUI_PIPELINES_VERSION=


# Enabled Open WebUI Pipelines
#
# Default=false
#
export OWUI_PIPELINES_ENABLED=true


# URL to the pipelines code to retrieve
#
export OWUI_PIPELINES_URLS="https://raw.githubusercontent.com/SUSE/suse-ai-observability-extension/refs/heads/main/integrations/oi-filter/suse_ai_filter.py"


# Name of the Open WebUI Service
#
# This is the name of the service that is diplayed in the Observability UI
# for the metrices gathered by the OpenTelemetry Collector from the OWUI 
# pipeline
#
# Default: Open WebUI
#
export OWUI_PIPELINES_OTEL_SERVICE_NAME="Open WebUI"


# Local Cluster OpenTelementry Collector Endpoint 
#
# The enpdoint of the OpenTelemetry Collector running in the local cluster 
# that the pipeline sends the metrics to.
#
# Note that this must contain port 4318 at the end of the URL.
#
export OWUI_PIPELINES_OTEL_EXPORTER_HTTP_OTLP_ENDPOINT="http://opentelemetry-collector.${OTEL_NAMESPACE}.svc.cluster.local:4318"


# URL to the Pricing File
#
# File that defines the pricing represented in SUSE Observability
#
export OWUI_PIPELINES_PRICING_JSON="https://github.com/SUSE/suse-ai-observability-extension/blob/main/integrations/oi-filter/pricing.json"


